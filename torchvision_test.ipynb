{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # v1 transforms API\n",
    "# from torchvision import transforms\n",
    "\n",
    "# trans = transforms.Compose([\n",
    "#     transforms.ColorJitter(contrast=0.5),\n",
    "#     transforms.RandomRotation(30),\n",
    "#     transforms.CenterCrop(480)\n",
    "# ])\n",
    "# imgs = trans(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # v2 transforms API\n",
    "# from torchvision.prototype import transforms\n",
    "# # Exactly the same interface as V1:\n",
    "# trans = transforms.Compose([\n",
    "#     transforms.ColorJitter(contrast=0.5),\n",
    "#     transforms.RandomRotation(30),\n",
    "#     transforms.CenterCrop(480),\n",
    "# ])\n",
    "# imgs, bboxes, labels = trans(imgs, bboxes, labels)\n",
    "\n",
    "# # Already supported:\n",
    "# trans(imgs)  # Image Classification\n",
    "# trans(videos)  # Video Tasks\n",
    "# trans(imgs_or_videos, labels)  # MixUp/CutMix-style Transforms\n",
    "# trans(imgs, bboxes, labels)  # Object Detection\n",
    "# trans(imgs, bboxes, masks, labels)  # Instance Segmentation\n",
    "# trans(imgs, masks)  # Semantic Segmentation\n",
    "# trans({\"image\": imgs, \"box\": bboxes, \"tag\": labels})  # Arbitrary Structure\n",
    "# # Future support:\n",
    "# trans(imgs, bboxes, labels, keypoints)  # Keypoint Detection\n",
    "# trans(stereo_images, disparities, masks)  # Depth Perception\n",
    "# trans(image1, image2, optical_flows, masks)  # Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'features' from 'torchvision' (/Users/pc2005/opt/anaconda3/envs/exp_pt/lib/python3.9/site-packages/torchvision/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/pc2005/workdir/pytorch_play/torchvision_test.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pc2005/workdir/pytorch_play/torchvision_test.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPIL\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pc2005/workdir/pytorch_play/torchvision_test.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pc2005/workdir/pytorch_play/torchvision_test.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m features, transforms \u001b[39mas\u001b[39;00m T\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pc2005/workdir/pytorch_play/torchvision_test.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pc2005/workdir/pytorch_play/torchvision_test.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Defining and wrapping input to appropriate Tensor Subclasses\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'features' from 'torchvision' (/Users/pc2005/opt/anaconda3/envs/exp_pt/lib/python3.9/site-packages/torchvision/__init__.py)"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "from torchvision import utils\n",
    "from torchvision import features, transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "# Defining and wrapping input to appropriate Tensor Subclasses\n",
    "path = \"COCO_val2014_000000418825.jpg\"\n",
    "img = features.Image(io.read_image(path), color_space=features.ColorSpace.RGB)\n",
    "# img = PIL.Image.open(path)\n",
    "bboxes = features.BoundingBox(\n",
    "    [[2, 0, 206, 253], [396, 92, 479, 241], [328, 253, 417, 332],\n",
    "     [148, 68, 256, 182], [93, 158, 170, 260], [432, 0, 438, 26],\n",
    "     [422, 0, 480, 25], [419, 39, 424, 52], [448, 37, 456, 62],\n",
    "     [435, 43, 437, 50], [461, 36, 469, 63], [461, 75, 469, 94],\n",
    "     [469, 36, 480, 64], [440, 37, 446, 56], [398, 233, 480, 304],\n",
    "     [452, 39, 463, 63], [424, 38, 429, 50]],\n",
    "    format=features.BoundingBoxFormat.XYXY,\n",
    "    spatial_size=F.get_spatial_size(img),\n",
    ")\n",
    "labels = features.Label([59, 58, 50, 64, 76, 74, 74, 74, 74, 74, 74, 74, 74, 74, 50, 74, 74])\n",
    "# Defining and applying Transforms V2\n",
    "trans = T.Compose(\n",
    "    [\n",
    "        T.ColorJitter(contrast=0.5),\n",
    "        T.RandomRotation(30),\n",
    "        T.CenterCrop(480),\n",
    "    ]\n",
    ")\n",
    "img, bboxes, labels = trans(img, bboxes, labels)\n",
    "# Visualizing results\n",
    "viz = utils.draw_bounding_boxes(F.to_image_tensor(img), boxes=bboxes)\n",
    "F.to_pil_image(viz).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('exp_pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21f6fb6ec0df82d9b822214a2115beb7277bde8576d936310f59810ee15b202c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
