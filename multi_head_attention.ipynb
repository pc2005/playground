{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(128, 64, 512)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=512\n",
    "n_head = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m     33\u001b[0m attention \u001b[38;5;241m=\u001b[39m multi_head_attention(d_model, n_head)\n\u001b[0;32m---> 34\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(output, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pt2/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pt2/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m, in \u001b[0;36mmulti_head_attention.forward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m     23\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones(time, time, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m))\n\u001b[1;32m     24\u001b[0m score \u001b[38;5;241m=\u001b[39m score\u001b[38;5;241m.\u001b[39mmasked_fill(mask\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 25\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39msoftmax(score) \u001b[38;5;241m@\u001b[39m v\n\u001b[1;32m     27\u001b[0m score \u001b[38;5;241m=\u001b[39m score\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch, time, dimension)\n\u001b[1;32m     28\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_combine(score)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pt2/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pt2/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class multi_head_attention(nn.Module):\n",
    "    def __init__(self, d_model, n_head) -> None :\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_combine = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        batch, time, dimension = q.shape\n",
    "        n_d = self.d_model // self.n_head\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "\n",
    "        # after permute: [batch, n_head, time, n_d]\n",
    "        q = q.view(batch, time, self.n_head, n_d).permute(0,2,1,3)\n",
    "        k = k.view(batch, time, self.n_head, n_d).permute(0,2,1,3)\n",
    "        v = v.view(batch, time, self.n_head, n_d).permute(0,2,1,3)\n",
    "\n",
    "        score = q @ k.transpose(2,3)/math.sqrt(n_d)\n",
    "        mask = torch.tril(torch.ones(time, time, dtype=bool))\n",
    "        score = score.masked_fill(mask==0, float(\"-inf\"))\n",
    "        score = self.softmax(score) @ v\n",
    "\n",
    "        score = score.permute(0,2,1,3).contiguous().view(batch, time, dimension)\n",
    "        output = self.w_combine(score)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "attention = multi_head_attention(d_model, n_head)\n",
    "output = attention(X, X, X)\n",
    "print(output, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-7.0129e-01, -5.7213e-01,  3.0061e-01,  ..., -1.7718e-01,\n",
      "          -9.2418e-02,  7.8293e-02],\n",
      "         [-2.7831e-01, -5.7390e-01, -1.2027e-01,  ...,  5.1195e-02,\n",
      "          -1.9941e-01,  1.9650e-01],\n",
      "         [-2.0199e-01, -3.9713e-01, -7.6728e-02,  ...,  9.7906e-02,\n",
      "          -1.7641e-01,  3.4393e-01],\n",
      "         ...,\n",
      "         [ 8.0917e-03,  2.2210e-02,  3.7414e-02,  ...,  1.2549e-02,\n",
      "          -7.5643e-02, -2.4918e-02],\n",
      "         [ 3.9393e-02,  2.3743e-02,  5.3089e-02,  ...,  9.8831e-03,\n",
      "          -4.2608e-02,  1.2276e-04],\n",
      "         [ 2.3659e-02,  1.4782e-02,  4.4167e-02,  ...,  3.6256e-02,\n",
      "          -4.3406e-02, -4.8235e-03]],\n",
      "\n",
      "        [[-5.8673e-01,  1.3055e-01,  5.7295e-01,  ...,  5.2459e-01,\n",
      "           1.2129e-01, -5.1063e-01],\n",
      "         [-3.9818e-01,  2.4426e-01,  4.7797e-02,  ...,  2.3442e-01,\n",
      "           3.0085e-01, -3.2239e-01],\n",
      "         [-2.7068e-01,  2.2138e-01,  2.4531e-02,  ...,  1.8273e-01,\n",
      "           1.8896e-01, -2.7297e-01],\n",
      "         ...,\n",
      "         [ 5.9622e-04,  2.4058e-02,  7.3588e-02,  ...,  3.0623e-02,\n",
      "          -2.3485e-02, -3.7168e-02],\n",
      "         [-3.6031e-03,  2.0161e-02,  6.5156e-02,  ...,  2.8514e-02,\n",
      "          -4.5571e-02, -3.6578e-02],\n",
      "         [-7.2112e-04,  3.5961e-02,  7.0029e-02,  ...,  1.4393e-02,\n",
      "          -4.0378e-02, -5.7920e-02]],\n",
      "\n",
      "        [[-1.7885e-01,  4.3356e-01, -3.7172e-01,  ...,  4.6765e-01,\n",
      "          -9.3610e-02,  2.4971e-01],\n",
      "         [-1.8376e-01, -7.9246e-02,  1.3352e-01,  ..., -1.2130e-02,\n",
      "          -2.0730e-01, -1.4611e-01],\n",
      "         [-2.4199e-01, -1.4480e-02,  1.0502e-01,  ...,  4.2136e-02,\n",
      "          -2.6417e-01, -1.2184e-01],\n",
      "         ...,\n",
      "         [-1.1440e-02, -2.3976e-02,  3.1786e-02,  ...,  4.9737e-02,\n",
      "           8.1837e-03, -1.1872e-02],\n",
      "         [-3.0584e-02, -2.2698e-02,  1.7649e-02,  ...,  5.6688e-02,\n",
      "           3.1810e-03, -6.8257e-03],\n",
      "         [-4.5659e-02, -4.6270e-02,  2.7492e-02,  ...,  6.0969e-02,\n",
      "           1.3074e-02, -2.2053e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.1416e-01, -2.7805e-02, -4.1053e-01,  ..., -6.0866e-02,\n",
      "          -9.5946e-02,  2.5713e-01],\n",
      "         [ 4.3825e-02,  4.6753e-01, -5.3084e-02,  ...,  1.9759e-01,\n",
      "          -2.6436e-01,  1.2806e-01],\n",
      "         [ 1.6835e-02,  2.8222e-01, -3.8798e-03,  ...,  1.5474e-01,\n",
      "          -2.8085e-01, -1.4846e-01],\n",
      "         ...,\n",
      "         [-3.3690e-02,  4.2590e-03,  7.4760e-05,  ...,  2.1796e-02,\n",
      "           1.0038e-03, -4.1217e-02],\n",
      "         [-5.2918e-02, -1.1570e-02, -1.3470e-02,  ...,  1.6120e-02,\n",
      "          -4.7515e-03, -3.9599e-02],\n",
      "         [-3.4036e-02, -1.2505e-02, -1.4326e-02,  ...,  2.1501e-02,\n",
      "          -2.4339e-02, -8.6090e-02]],\n",
      "\n",
      "        [[ 6.2124e-01, -4.3765e-01,  3.2984e-01,  ..., -1.1002e-02,\n",
      "           1.6277e-01, -3.3474e-03],\n",
      "         [ 1.1516e-01, -4.5436e-01,  1.5706e-01,  ...,  3.4481e-02,\n",
      "           3.0165e-01, -7.3724e-02],\n",
      "         [-8.1101e-02, -2.3996e-01, -5.7139e-03,  ...,  4.7129e-02,\n",
      "           4.7433e-01, -6.4857e-02],\n",
      "         ...,\n",
      "         [ 3.3493e-02,  3.4317e-02,  4.4595e-02,  ...,  2.7831e-02,\n",
      "          -2.6120e-02,  2.5129e-02],\n",
      "         [ 1.0709e-02,  2.4524e-02,  5.7751e-02,  ...,  1.6334e-02,\n",
      "           1.3641e-02,  4.6153e-02],\n",
      "         [ 1.3328e-02, -2.6619e-03,  2.9140e-02,  ...,  7.3021e-02,\n",
      "          -1.5668e-02,  2.3754e-02]],\n",
      "\n",
      "        [[-4.1366e-01, -2.0244e-01, -2.0513e-01,  ..., -4.5726e-01,\n",
      "          -3.6699e-01, -5.8488e-01],\n",
      "         [-3.7633e-01, -7.1480e-02,  2.7709e-01,  ...,  7.1871e-02,\n",
      "           6.4453e-02, -4.2127e-01],\n",
      "         [-3.0262e-01, -3.8016e-02,  3.0093e-01,  ..., -2.4563e-02,\n",
      "           4.0226e-02, -3.7743e-01],\n",
      "         ...,\n",
      "         [ 3.6923e-02,  6.5846e-02, -5.9173e-02,  ...,  6.7973e-02,\n",
      "          -7.1517e-02,  3.9470e-02],\n",
      "         [ 5.6003e-02,  5.1923e-02, -1.1085e-01,  ...,  6.8743e-02,\n",
      "          -5.9908e-02,  4.0623e-02],\n",
      "         [ 7.0072e-02,  5.3358e-02, -8.4300e-02,  ...,  4.6942e-02,\n",
      "          -5.0860e-02,  4.2843e-02]]], grad_fn=<ViewBackward0>) torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "attention = multi_head_attention(d_model, n_head)\n",
    "output = attention(X, X, X)\n",
    "print(output, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
